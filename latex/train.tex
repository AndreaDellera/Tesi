
\section{Train della rete}
Per allenare la RNN \`e stato scelto l'algoritmo di Back Propagation, associato a Gradient Descent. 
L'aggiornamento dei pesi viene fatto in batchlearning, ovvero alla fine di tutte le epoch, per la rete neurale ricorrente e on-line, cio\`e alla fine di ogni epoch, per la rete neurale non ricorrente.
Sono stati utilizzati diversi valori di learning rate e di momentum, due parametri che influiscono su come la rete \`e allenata;
il primo \`e una costante che decide la quantit\`a di cambiamento sui vecchi pesi all'interno della rete mentre il secondo comporta una ulteriore adattamento dei pesi in base ai cambiamenti precedenti e permette di avvicinarsi pi\`u velocemente al minimo.\\
Dopo diverse prove sono stati utilizzati valori i seguenti valori:
\begin{table}[ht]
\centering
\begin{tabular}{| c | c | c |}
\multicolumn {3}{c}{}\\
\hline
&\textbf{FFN}&\textbf{RNN}\\\hline
Learning Rate&0.3&0.1\\\hline
Momentum&0.9&0.3\\\hline
\end{tabular}
\end{table}
\\
Per il valore del learning rate \`e stata adottata la tecnica Bold Driver~\cite{battiti1989accelerated} che ne cambia la quantit\`a a seconda di come procede l'apprendimento da parte della rete.
La funzione di errore da minimizzare durante il la fase di train \`e quello dei minimi quadrati.
Per allenare la rete \`e stata utilizzata la tecnica di k-fold cross validation~\cite{kohavi1995study}, dividendo il database iniziale in dieci parti ed utilizzandone nove per il train e una per la validazione. Il codice riportato \`e la funzione di train utilizzata per allenare la rete; \`e stata modificata dal codice originale per adattarsi meglio al programma e per implementare la k-fold cross validation.
\\
\begin{lstlisting}[language=Python, caption=Train Until Convergence]
trainingData = datasetTrain
validationData = datasetTest
self.ds = trainingData
bestweights = self.module.params.copy()
bestverr = self.testOnData(validationData)
trainingErrors = []
validationErrors = [bestverr]
while True:
	trainingErrors.append(self.train())
	validationErrors.append(self.testOnData(validationData))
	if validationErrors[-1] < bestverr:
 		# one update is always done
		bestverr = validationErrors[-1]
		bestweights = self.module.params.copy()

	if maxEpochs is not None and epochs >= maxEpochs:
		self.module.params[:] = bestweights
		break
	epochs += 1

	if len(validationErrors) >= continueEpochs * 2:
		old = validationErrors[-continueEpochs * 2:-continueEpochs]
		new = validationErrors[-continueEpochs:]
		if min(new) > max(old):
			self.module.params[:] = bestweights
			break
return trainingErrors, validationErrors
\end{lstlisting}
La funzione salva lo stato dei pesi della rete quando, in fase di validazione, viene ottenuto un errore minore al minimo. Inoltre se l'errore comincia a salire nuovamente controlla di quanto lo fa e, in caso, blocca il train settando i pesi a quelli che davano errore di validazione minore e ritorna la progressione degli errori in fase di train e validazione.
Per concludere questa sezione \`e da notare che \`e stata valutata l'ipotesi di usare l'algoritmo di Back Propagation Through Time~\cite{werbos1990backpropagation}, versione particolare di Back Propagation adattata alle RNN, ma non era presente nel pacchetto utilizzato per implementare le reti~\cite{schaul2010pybrain} e la complessit\`a dell'implementazione l'ha resa infattibile.